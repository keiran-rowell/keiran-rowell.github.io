---
layout: post
author: keiran
title: "Burning Jacob's Ladder"
subtitle: "Molecular Structure Prediction in the Age of Deep Neural Nets and Ultrafast GPUs"
category: compchem 
tags: AlphaFold AI Deep-learning Tech Transformers neural-nets 
---

*Ashes scatter
Burning Jacob's ladder
I'm afraid I might fade away
*
---

Why climb a ladder of increasingly complex approximations when Deep Learning approaches just learn the patterns in emergent phenomena directly at the level they manifest? Connecting things back to their true ab initio processes are a bit of a fiction, so why no simulate the stuff that matters to our physically interacting microscale world?

ML potentials are kinda the killer demonstration to me. Look your MD is basically sine waves for balls on springs. Machine Learning actual captures the bumping and librations of real biomolecular residues better and without constant cross-fudging parameters. The cross terms of force-fields and lack of polarisability never quite work.

It's also an ideal demonstration set for gradient descent on a real potential energy surface, this time quite complex because they are biomolecules.

I could even do a QM PES scan and show a force-field fit and a DL fit. It would teach the concepts of the approaches 

- Kohn-Sham theorems as existence proofs. There exists so completely accurate universal exchange correlation functional that maps from density to energies. But nobody knows its form. There are been decades of attempts to construct better exchange-correlation (XC) kernels, starting from Jellium and then [climbing Jacob's Ladder](). In fact most DFT methods differ by the XC kernel used, leading to projects like [libXC](https://libxc.gitlab.io/) to make portable the effort of reimplementing functionals into working code.
- Density Functional Theory has already been lamented as a [Frankenstein monsteresque hybrid once it lost its density purity].(https://doi.org/10.1071/CH02049)
- Density functionals that return better energetic prediction values are actually [performing worse on their fundemental properties[(https://cen.acs.org/articles/95/i2/Density-functional-theory-heads-wrong.html), predicting the electron density itself.
- Google DeepMind using a [neural network to refine a new DFT function](https://research.google/pubs/learning-to-approximate-density-functionals/).
- Facebook AI Research Labs releasing massive neural net molecular dataset, [OMol25](https://ai.meta.com/blog/meta-fair-science-new-open-source-releases/).
- The dubiously grounded inferred Molecular Dyanmics of [BioEMU](https://doi.org/10.1126/science.adv9817)
- Machine learning force-fields ["Molecular Simulations with a Pretrained Neural Network and Universal Pairwise Force Field"](https://doi.org/10.1021/jacs.5c09558)
- GPU accelerated AI powered chemistry with NVIDIA:
	- 
- GPU accelerated AI powered chemistry with NVIDIA:
   - https://developer.nvidia.com/blog/accelerating-ai-powered-chemistry-and-materials-science-simulations-with-nvidia-alchemi-toolkit-ops/https://developer.nvidia.com/blog/accelerating-ai-powered-chemistry-and-materials-science-simulations-with-nvidia-alchemi-toolkit-ops/
- ["Platonic representation of foundation machine learning interatomic potentials"](https://arxiv.org/abs/2512.05349) and ["Universally Converging Representations of Matter Across Scientific Foundation Models"](https://arxiv.org/abs/2512.03750)
- AlphaFold
- RFDiffusion
- Flow matching (LaProteina)
- OMRTA: [A Multi-Task Generative Model for Structure-Based Drug Design](https://arxiv.org/abs/2512.05080)	
